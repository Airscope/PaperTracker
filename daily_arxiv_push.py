import requests
import feedparser
from datetime import datetime, timezone, timedelta
import os

# é…ç½®å…³é”®è¯ç»„åˆ
KEYWORDS = [
    "LLM", '"large language model"', "agent"
]
# é¡¶ä¼šå…³é”®è¯åˆ—è¡¨
TOP_CONFS = ["ICML", "ACL", "NIPS", "Neurips", "ICLR", "CVPR", "AAAI", "ECCV", "ICCV", "TPAMI"]
# å¸¸è§ä¸­æ–‡å§“æ°ï¼Œç”¨äºåˆ¤æ–­ç¬¬ä¸€ä½œè€…æ˜¯å¦å¯èƒ½ä¸ºä¸­å›½äºº
CHINESE_SURNAMES = [
    "Zhao", "Qian", "Sun", "Li", "Zhou", "Wu", "Zheng", "Wang", "Feng", "Chen",
    "Chu", "Wei", "Jiang", "Shen", "Han", "Yang", "Zhu", "Qin", "You", "Xu",
    "He", "Lu", "Shi", "Zhang", "Kong", "Cao", "Yan", "Bai", "Shui", "Dou",
    "Zhang", "Yun", "Su", "Pan", "Ge", "Xi", "Fan", "Peng", "Lang", "Lu",
    "Wei", "Chang", "Ma", "Miao", "Feng", "Hua", "Fang", "Yu", "Ren", "Yuan",
    "Liu", "Xie", "Lei", "He", "Ni", "Tang", "Teng", "Yin", "Luo", "Bi",
    "Hao", "Wu", "An", "Chang", "Le", "Yu", "Shi", "Fu", "Pi", "Bian",
    "Qi", "Kang", "Wu", "Yu", "Yuan", "Bu", "Gu", "Meng", "Ping", "Huang",
    "He", "Mu", "Xiao", "Yin", "Xu", "Du", "Peng", "Shi", "Yun", "Zhong",
    "Luo", "Yan", "Shang", "Luo", "Huang", "Mu", "Xiao", "Yin", "Xu", "You"
]

# æ„é€  arXiv API æŸ¥è¯¢
def fetch_yesterday_llm_papers():
    today_utc = datetime.now(timezone.utc).date()
    yesterday_utc = today_utc - timedelta(days=1)

    query = "+OR+".join(f"all:{kw}" for kw in KEYWORDS)
    url = (
        f"http://export.arxiv.org/api/query?"
        f"search_query={query}&start=0&max_results=200"
        f"&sortBy=submittedDate&sortOrder=descending"
    )

    resp = requests.get(url)
    feed = feedparser.parse(resp.content)

    papers = []
    for entry in feed.entries:
        pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).date()
        if pub_date != yesterday_utc:
            continue

        title = " ".join(entry.title.split())
        authors = [a.name for a in entry.authors]
        first_author = authors[0] if authors else ""
        authors_str = ", ".join(authors)
        comment = getattr(entry, "arxiv_comment", "æ— å¤‡æ³¨")
        summary = entry.summary.strip().replace("\n", " ")
        summary_short = summary[:300] + ("..." if len(summary) > 300 else "")

        papers.append({
            "title": title,
            "authors": authors_str,
            "first_author": first_author,
            "comment": comment,
            "summary_short": summary_short,
            "summary": summary,
            "link": entry.link
        })

    return papers

# ä¸ºæ¯ç¯‡è®ºæ–‡è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
def score_paper(paper):
    score = 0
    c = paper['comment'].lower()
    summary = paper['summary']
    # accepted æ ‡è®°
    if 'accept' in c:
        score += 3
    # å¼€æºä»£ç 
    if 'github' in c or 'github' in summary.lower():
        score += 2
    # é¡¶ä¼š
    for conf in TOP_CONFS:
        if conf.lower() in c:
            score += 2
            break
    # ç¬¬ä¸€ä½œè€…åå­—éä¸­å›½å§“æ°
    parts = paper['first_author'].split()
    surname = parts[-1] if parts else ''
    if surname not in CHINESE_SURNAMES:
        score += 1
    return score

# æ„é€ é£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡æ ¼å¼
def build_feishu_card(papers, date_str):
    total = len(papers)
    # æŒ‰åˆ†æ•°é™åºæ’åºï¼Œå–å‰10
    ranked = sorted(papers, key=score_paper, reverse=True)[:10]

    header_title = f"ğŸ“š æ˜¨æ—¥ ({date_str}) å…±æ›´æ–° {total} ç¯‡ LLM è®ºæ–‡ï¼Œä¼˜å…ˆå±•ç¤º Top 10"
    elements = []
    for idx, paper in enumerate(ranked, 1):
        content = (
            f"**{idx}. æ ‡é¢˜ï¼š** {paper['title']}\n"
            f"**ä½œè€…ï¼š** {paper['authors']}\n"
            f"**å¤‡æ³¨ï¼š** {paper['comment']}\n"
            f"**æ‘˜è¦ï¼š** {paper['summary_short']}\n"
            f"[ğŸ”— æŸ¥çœ‹åŸæ–‡]({paper['link']})"
        )
        elements.append({
            "tag": "div",
            "text": {"tag": "lark_md", "content": content}
        })

    card = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": header_title}},
            "elements": elements
        }
    }
    return card

# å‘é€åˆ°é£ä¹¦æœºå™¨äºº Webhook
def send_to_feishu(card_json):
    webhook = os.environ.get("FEISHU_WEBHOOK")
    if not webhook:
        raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ FEISHU_WEBHOOK")

    resp = requests.post(webhook, json=card_json)
    if resp.status_code != 200:
        print("âŒ é£ä¹¦æ¨é€å¤±è´¥:", resp.status_code, resp.text)
    else:
        print(f"âœ… æ¨é€æˆåŠŸï¼Œå±•ç¤º Top {len(card_json['card']['elements'])} ç¯‡è®ºæ–‡")


def main():
    papers = fetch_yesterday_llm_papers()
    date_str = (datetime.now(timezone.utc).date() - timedelta(days=1)).isoformat()
    card = build_feishu_card(papers, date_str)
    send_to_feishu(card)

if __name__ == "__main__":
    main()
